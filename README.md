# TEKO Vision-Based Docking System

This repository is part of the master's thesis project
**"Adaptive Cooperation in Agricultural Robot Swarms: Reinforcement Learning and Evolutionary Algorithms for Modular Docking"**.

The work is being carried out at the **University of Hohenheim**,
Department of Artificial Intelligence in Agricultural Engineering,
under the supervision of **Prof. Dr. Anthony Stein** and **Dr. David Reiser**.

---

## 1. Project Overview

The TEKO project studies how small agricultural robots can **physically dock** to form modular units capable of performing tasks that would traditionally require larger, more complex machines.

This repository implements a **vision-only autonomous docking system**:

* A mobile TEKO robot must **locate, align, and connect** to a static TEKO goal robot.
* Perception is based purely on **RGB images** from a rear-mounted camera.
* Control is learned with **reinforcement learning (PPO)** in **NVIDIA Isaac Lab 0.47.1 / Isaac Sim 5.0**.
* The setup is designed to be later transferable to **real TEKO hardware**.

The code covers the full pipeline: CAD â†’ USD/URDF models â†’ simulated environment â†’ RL training â†’ logging and analysis.

---

## 2. Research Motivation

Modern agriculture faces:

* Labour shortages and high labour costs
* Increased climate variability and production risks
* Pressure to reduce inputs and environmental impact

Traditional solutions rely on **large, monolithic machines**, which are expensive and not always suitable for smaller or more diversified farms.

**Swarm and modular robotics** offer an alternative: many small, affordable units that can act **individually or cooperatively**. A key technical challenge is enabling **robust physical cooperation**, such as mechanical docking and resource sharing.

This project focuses on the **autonomous docking behaviour** itself, using vision-based RL to make two small robots **connect reliably without handcrafted docking sequences**.

---

## 3. Main Objectives

1. **Design and model** a docking-capable TEKO robot in simulation (USD/URDF, CAD-based geometry).
2. **Implement a realistic docking arena**, including a goal robot with an ArUco marker and a well-defined docking interface.
3. **Train a reinforcement learning agent** to perform docking using **only RGB input** from a rear camera.
4. **Introduce a multi-stage curriculum** that gradually increases task difficulty (distance, lateral offset, orientation).
5. **Prepare the pipeline for evolutionary hyperparameter optimisation** and later **sim-to-real transfer**.

---

## 4. System Overview

### 4.1 Simulation and Robot Models

The simulation is implemented in **NVIDIA Isaac Lab 0.47.1** (Isaac Sim 5.0):

* TEKO robot exported from **Fusion 360** as meshes and assembled into USD/URDF.
* The robot includes:
  * Chassis, four differential-drive wheels, body, roof and sensor mounts.
  * Back-mounted **camera module** emulating the Raspberry Pi Camera Module 2.
  * A **rear connector** (male/female) used for mechanical docking.
* A separate **static TEKO goal** is spawned as the docking target, equipped with:
  * An **ArUco marker** in front of the connector.
  * Spheres on both robots to define a geometric **docking distance**.

The arena (`stage_arena.usd`) defines walls and floor, constraining the robot to a controlled region.

### 4.2 Docking Geometry and Ground Truth

Docking quality is measured using **virtual spheres** placed in the connectors of both robots. The environment computes:

* **3D distance** between the connector spheres,
* **Projected XY distance** on the ground plane (`surface_xy`),

and uses these distances for:

* **Reward shaping** (distance, progress, proximity),
* **Success detection** (dock if `surface_xy < 0.03 m`),
* **Collision detection** (too fast / too close â†’ heavy penalty).

This keeps the learning signal **geometric and consistent**, independent of the camera artefacts.

---

## 5. Reinforcement Learning Setup

### 5.1 Observations

* **Modality:** RGB images from the **rear camera** of the mobile robot.
* **Resolution:** `640 Ã— 480` (3 channels, `float32` in `[0, 1]`).
* **Viewpoint:** The rear camera looks toward the docking interface and ArUco marker when the robot is correctly positioned.

### 5.2 Action Space

The policy outputs a **2D continuous action vector**:

* `v_cmd` â€“ forward/backward command (linear component)
* `w_cmd` â€“ turning command (angular component)

These commands are then **mapped inside the environment** to **wheel torques** for the left and right wheel pairs.

Previously, the agent directly produced `[left_torque, right_torque]`. While this is also 2D, it implicitly allowed many unstructured combinations (weak left / strong right, asymmetric strong torques, etc.), and the network had to discover the underlying structure on its own.

The new `[v_cmd, w_cmd]` parameterisation:

* Encodes the **natural structure of differential drive**,
* Makes the control space **more interpretable**,
* Encourages **smoother and more consistent** docking behaviour,
* Still remains a **continuous** action space.

### 5.3 Reward Function

The reward (see `reward_functions.py`) combines:

1. **Distance reward** â€“ linear penalty with respect to connector distance.
2. **Progress reward** â€“ positive when the robot reduces distance to the goal.
3. **Alignment reward** â€“ based on the **rear of the robot** facing the goal; the yaw of the rear connector is aligned to the vector from robot to goal.
4. **Velocity penalty** â€“ discourages excessive speed in the plane.
5. **Oscillation penalty** â€“ penalises large changes in actions between timesteps.
6. **Collision penalty** â€“ large negative reward when the robot approaches too fast and "crashes" instead of docking (anti-exploit).
7. **Boundary penalty** â€“ large negative reward when leaving the arena.
8. **Success bonus** â€“ strong positive reward when docking is successful (`surface_xy < 3 cm`).
9. **Proximity bonus** â€“ extra reward when very close but not yet docked.
10. **Survival bonus** â€“ small per-step reward to make "surviving and trying" better than crashing early.

This design encourages the agent to dock **quickly but safely**, rather than exploiting collisions or walls.

### 5.4 Policy Network and Visual Encoder

The policy is implemented in pure **PyTorch** (no external RL frameworks at runtime):

* **Encoder:** configurable CNN defined in `cnn_model.py`.
* **Actor head:** MLP mapping visual features â†’ `[v_cmd, w_cmd]` (Tanh).
* **Critic head:** MLP mapping visual features â†’ state value estimate.

Two encoder options exist:

1. **SimpleCNN (default)**
   * Lightweight, custom CNN with several Conv2D + pooling blocks.
   * Automatically adapts to input size (`480 Ã— 640`).
   * Initialised with Kaiming/Xavier schemes.
   * Designed for **stability and low memory usage** with many parallel environments.

2. **MobileNetV3-Small (optional)**
   * Pretrained on ImageNet via `torchvision.models.mobilenet_v3_small`.
   * Provides strong visual feature extraction out of the box.
   * More memory-intensive; useful for **transfer-learning experiments** or encoder comparisons.

> **Why SimpleCNN as default?**
> Earlier tests with a pretrained MobileNetV3-Small backbone significantly increased GPU memory usage and occasionally led to out-of-memory errors when training with many parallel environments on the RTX 3090. The SimpleCNN keeps the model compact, avoids memory issues with 16 environments, and still provides sufficiently rich features for learning the docking behaviour. MobileNetV3-Small remains available in `cnn_model.py` for future experiments targeting transfer learning or detailed encoder comparisons.

### 5.5 PPO Algorithm

The training loop in `scripts/skrl/train_curriculum.py` implements **Proximal Policy Optimization (PPO)** with:

* **GAE(Î»)** advantage estimation,
* Clipped policy objective,
* Optional value clipping,
* Entropy regularisation,
* Gradient clipping,
* Support for **checkpointing** and **TensorBoard logging**.

Core hyperparameters (Î³, Î», clipping, entropy/value coefficients, etc.) are centralised in a `HYPERPARAMS` dictionary, simplifying **future genetic/evolutionary optimisation**.

---

## 6. Curriculum Learning

Docking is trained via a **16-stage ultra-gradual curriculum** (`curriculum_manager.py`):

* Early stages: robot starts **very close and aligned**, learning basic backward motion into the connector.
* Intermediate stages: gradually increase **distance**, **lateral offset**, and **yaw misalignment**.
* Later stages: include **180Â° misalignment** and **far starting positions**, requiring turning, search and re-alignment.
* Final stage: **"Full Autonomy"** â€“ robot starts anywhere in a large area around the goal with random yaw.

Stage progression:

* The trainer tracks success rate per stage.
* A stage is eligible for transition when:
  * A **minimum number of steps** has been executed in the current stage (e.g. 15,000 steps), **and**
  * The stage success rate reaches a predefined threshold (e.g. 85 %).

Curriculum control is implemented jointly by:

* `curriculum_manager.py` â€“ defines spawn distributions per stage.
* `train_curriculum.py` â€“ implements stage advancement logic based on metrics.

---

## 7. Repository Structure

Aligned with the current project tree:
```text
TEKO/
â”œâ”€â”€ README.md                     â† Project documentation (this file)
â”œâ”€â”€ _cam_out/                     â† Camera debug output (if used)
â”œâ”€â”€ debug_frames/
â”‚   â””â”€â”€ verification_frame.png    â† Docking geometry & camera sanity checks
â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ Aruco/                    â† ArUco marker textures
â”‚   â”œâ”€â”€ CAD/                      â† Fusion 360 exports (meshes + URDF)
â”‚   â”‚   â”œâ”€â”€ Other_Formats/
â”‚   â”‚   â”‚   â”œâ”€â”€ stage_1/          â† Stage arena assets
â”‚   â”‚   â”‚   â””â”€â”€ teko/             â† TEKO robot meshes & URDF
â”‚   â”‚   â””â”€â”€ USD/
â”‚   â”‚       â”œâ”€â”€ stage_arena.usd   â† Docking arena
â”‚   â”‚       â”œâ”€â”€ teko.usd          â† TEKO robot (mobile)
â”‚   â”‚       â””â”€â”€ teko_goal.usd     â† TEKO goal (static)
â”‚   â”œâ”€â”€ error.txt                 â† Misc. notes / debug info
â”‚   â””â”€â”€ pictures/                 â† Figures for thesis & documentation
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ dockin_aruco.py           â† ArUco-based docking experiments (utility)
â”‚   â”œâ”€â”€ straight.py               â† Simple motion tests
â”‚   â”œâ”€â”€ test_env.py               â† Environment sanity checks
â”‚   â”œâ”€â”€ visualize_docking_points.py â† Visualisation of connector spheres
â”‚   â””â”€â”€ skrl/                     â† RL training scripts (custom PPO)
â”‚       â”œâ”€â”€ debug.py
â”‚       â”œâ”€â”€ red_dots.py
â”‚       â”œâ”€â”€ train_curriculum.py   â† MAIN training entrypoint (16-stage curriculum)
â”‚       â”œâ”€â”€ train_curriculum_until_s4.py â† Early-stage tests
â”‚       â”œâ”€â”€ train_manual.py       â† Manual / non-curriculum experiments
â”‚       â””â”€â”€ train_production.py   â† Older production-style training script
â”‚
â”œâ”€â”€ source/
â”‚   â””â”€â”€ teko/
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ extension.toml    â† Isaac Lab extension config
â”‚       â”œâ”€â”€ docs/
â”‚       â”‚   â””â”€â”€ CHANGELOG.rst
â”‚       â”œâ”€â”€ pyproject.toml
â”‚       â”œâ”€â”€ setup.py
â”‚       â”œâ”€â”€ teko/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ ui_extension_example.py
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â”‚   â””â”€â”€ direct/
â”‚       â”‚   â”‚       â””â”€â”€ teko/
â”‚       â”‚   â”‚           â”œâ”€â”€ teko_env.py         â† Environment implementation
â”‚       â”‚   â”‚           â”œâ”€â”€ teko_env_cfg.py     â† Environment configuration (camera, robots, sim)
â”‚       â”‚   â”‚           â”œâ”€â”€ curriculum/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ curriculum_manager.py â† 16-stage curriculum
â”‚       â”‚   â”‚           â”œâ”€â”€ rewards/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ reward_functions.py   â† Sphere-based docking rewards
â”‚       â”‚   â”‚           â”œâ”€â”€ penalties/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ penalties.py          â† Legacy/experimental penalties (currently unused)
â”‚       â”‚   â”‚           â”œâ”€â”€ robots/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ teko.py               â† Dynamic TEKO articulation configuration
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ teko_static.py        â† Static TEKO goal configuration
â”‚       â”‚   â”‚           â”œâ”€â”€ sensors/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ camera.py
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ imu.py
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ lidar.py
â”‚       â”‚   â”‚           â”œâ”€â”€ teko_brain/
â”‚       â”‚   â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚           â”‚   â””â”€â”€ cnn_model.py          â† SimpleCNN + MobileNetV3 encoders
â”‚       â”‚   â”‚           â””â”€â”€ utils/
â”‚       â”‚   â”‚               â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚               â”œâ”€â”€ geometry_utils.py     â† Sphere distances, transforms, etc.
â”‚       â”‚   â”‚               â””â”€â”€ logging_utils.py      â† Reward component logging helpers
â”‚       â””â”€â”€ teko.egg-info/   â† Python package metadata (generated)
â”‚
â”œâ”€â”€ teko_curriculum/             â† TensorBoard logs & checkpoints
â”‚   â”œâ”€â”€ YYYYMMDD_HHMMSS/
â”‚   â”‚   â”œâ”€â”€ events.out.tfevents.*   â† Training logs
â”‚   â”‚   â””â”€â”€ final.pt                â† Final model checkpoint
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ spawn_positions_12stage.png  â† Visualisation of spawn positions (older design)
â”œâ”€â”€ test_env_basic.py            â† Minimal environment smoke test
â””â”€â”€ train_pid.txt                â† PID experiments / notes (if used)
```

---

## 8. Training Workflow

### 8.1 Launching Isaac Lab (headless PPO training)

From the repository root:
```bash
/workspace/isaaclab/_isaac_sim/python.sh \
  scripts/skrl/train_curriculum.py \
  --num_envs 16 \
  --steps 2000000 \
  --headless
```

Common options:

* `--num_envs` : number of parallel environments (e.g. 8, 16).
* `--steps`    : total environment steps to train.
* `--lr`       : learning rate (default `1e-4`).
* `--rollout_len` : rollout horizon per PPO update (default `64`).
* `--epochs`   : PPO epochs per update (default `8`).
* `--batch_size` : minibatch size for PPO.
* `--checkpoint` : path to a `.pt` checkpoint to resume from.

### 8.2 Monitoring Training

TensorBoard logs are written under `teko_curriculum/`:
```bash
tensorboard --logdir teko_curriculum
```

Main metrics:

* `train/reward` â€“ mean episode reward
* `train/episode_length` â€“ mean episode length
* `train/success_rate` â€“ recent docking success rate
* `train/stage_success` â€“ success rate within the current curriculum stage
* `train/curriculum_stage` â€“ current stage index (0â€“15)
* Policy/value losses and entropy

### 8.3 Checkpoints

The trainer periodically saves:

* `ckpt_*.pt` â€“ intermediate checkpoints with:
  * Policy weights
  * Optimiser state
  * Current training step
  * Curriculum level and steps in current stage

* `final.pt` â€“ final model at the end of training.

These checkpoints can be used both for **evaluation** scripts and for **fine-tuning / continued training**.

---

## 9. Future Work

* **Evolutionary hyperparameter optimisation** (e.g. genetic algorithms) using the centralised `HYPERPARAMS` dictionary.
* Detailed comparison between **SimpleCNN** and **pretrained MobileNetV3** encoders in terms of sample efficiency, robustness and sim-to-real transfer.
* **Sim-to-Real** deployment on physical TEKO robots in the research hall, including:
  * Domain randomisation,
  * Sensor noise,
  * Real lighting and material effects.
* Extension from **single docking** to **multi-robot cooperative docking** and chained configurations.
* Integration with **ROS 2** for real-time control and logging.

---

## 10. Contact

**Alexandre Schleier Neves da Silva**  
M.Sc. Environmental Protection and Agricultural Food Production  
University of Hohenheim

ğŸ“§ [alexandre.schleiernevesdasilva@uni-hohenheim.de](mailto:alexandre.schleiernevesdasilva@uni-hohenheim.de)

---

This repository provides the experimental framework for the **TEKO Vision-Based Docking System**, forming the core of the master's thesis on **adaptive cooperation in modular agricultural robot swarms**.